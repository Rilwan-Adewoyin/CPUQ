{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyses for the SPOT Alignment Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import re\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "# import roc curve printing, roc_auc_score, brier_score_loss\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, brier_score_loss\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "def load_data(dir_expirements) -> list[dict]:\n",
    "    \"\"\"Experiments are stored as a csv file in the following format: budget_item,indicator,label,pred_aggregated,prompts,predictions,discourse\n",
    "        Each experiment also contains a config file\n",
    "\n",
    "        Returns a list of dictionaries, where \n",
    "\n",
    "        Input format of csv file containing predictions: budget_item,indicator,label,pred_aggregated,prompts,predictions,discourse\n",
    "       \"\"\"\n",
    "    \n",
    "    experiment_paths = sorted(glob.glob(dir_expirements+'/*'))\n",
    "    li_exp = []\n",
    "    for path in experiment_paths:\n",
    "        path_config = os.path.join(path,'config.yaml')\n",
    "        path_results = os.path.join(path,'predictions_b2i.csv')\n",
    "\n",
    "        # Load config file\n",
    "        with open(path_config) as f:\n",
    "            config   =  yaml.safe_load(f)\n",
    "              \n",
    "        # Load results file\n",
    "        df = pd.read_csv(path_results)\n",
    "\n",
    "        li_exp.append( {'config':config, 'results':df} )\n",
    "    \n",
    "    return li_exp\n",
    "\n",
    "def filter_exp( li_exp: list[dict], **filter_kwargs ) -> list[dict]:\n",
    "\n",
    "    li_exp_filtered = []\n",
    "\n",
    "    # first filter logic based on checking if any item in  the filter_kwarg is a substring of the config value\n",
    "    for exp in li_exp:\n",
    "        if 'llm_names' in filter_kwargs:\n",
    "            if any( llm_name in exp['config']['llm_name'].lower() for llm_name in filter_kwargs['llm_names'] ):\n",
    "                li_exp_filtered.append(exp)\n",
    "        else:\n",
    "            li_exp_filtered.append(exp)\n",
    "    \n",
    "    filter_kwargs.pop('llm_names', None)\n",
    "\n",
    "    # second filter logic based on checking if any item in list is in the config    \n",
    "    for idx in range(len(li_exp_filtered)-1, -1, -1):\n",
    "        exp = li_exp_filtered[idx]\n",
    "        if any( ( (exp['config'].get(filter_name) not in filter_values) for filter_name, filter_values in filter_kwargs.items() ) ):\n",
    "            # li_exp_filtered.append(exp)\n",
    "            li_exp_filtered.pop(idx)\n",
    "\n",
    "    return li_exp_filtered\n",
    "\n",
    "def calc_eval_metrics( li_exp, metrics:list = ['accuracy','precision','recall','f1','roc_auc','brier_score','mae'], breakdown_by_budgetitem:bool=False, average_type=None ) -> dict:\n",
    "    \"\"\"\n",
    "        kwargs must be: keys,values = config argument, list of values to filter on\n",
    "\n",
    "        Filters experiments based on kwargs, then calculates metrics for each experiment\n",
    "    \"\"\"\n",
    "    \n",
    "    li_exp = copy.deepcopy(li_exp)\n",
    "\n",
    "    # Calculate metrics for each experiment\n",
    "    tgt_col = 'related'\n",
    "    pred_col_dict = 'pred_aggregated'\n",
    "    pred_col_label = 'pred_label' # For deterministic evaluation\n",
    "    # pred_col_prob = 'pred_prob'\n",
    "    \n",
    "    ## Making pred_label column\n",
    "    for exp in li_exp:\n",
    "\n",
    "        exp['results'][pred_col_label] = exp['results'][pred_col_dict].apply(lambda dict_: max(eval(dict_).items(), key=operator.itemgetter(1))[0])\n",
    "    \n",
    "    if 'roc_auc' in metrics or 'brier_score' in metrics or 'calibration_error' or 'mae' in metrics:\n",
    "        # Making pred_prob column e.g. Prob of predicting Yes\n",
    "        for exp in li_exp:\n",
    "            #note: the logic in the below line may be  a bit wrong, since it extracts the pred_col_prob of highest value, instead of the pred_col_prob for the label 'Yes' or 'No'\n",
    "            # exp['results'][pred_col_prob] = exp['results'][pred_col_dict].apply(lambda dict_: max(eval(dict_).items(), key=operator.itemgetter(1))[1])\n",
    "\n",
    "            exp['results']['pred_yes_prob'] = exp['results'][pred_col_dict].apply(lambda dict_: eval(dict_)['Yes'])\n",
    "            exp['results']['pred_no_prob'] = exp['results'][pred_col_dict].apply(lambda dict_: eval(dict_)['No'])\n",
    "\n",
    "            exp['results'][tgt_col+'_yes_prob'] = exp['results'][tgt_col].map(lambda x: 1.0 if x=='Yes' else 0.0)\n",
    "            exp['results'][tgt_col+'no_prob'] = exp['results'][tgt_col].map(lambda x: 1.0 if x=='No' else 0.0)          \n",
    "\n",
    "\n",
    "    if breakdown_by_budgetitem is False:\n",
    "        ## Accuracy\n",
    "        if 'accuracy' in metrics:\n",
    "            for exp in li_exp:\n",
    "                # Calculate accuracy\n",
    "                accuracy = accuracy_score(exp['results'][tgt_col], exp['results'][pred_col_label])\n",
    "                if 'metrics' in exp:\n",
    "                    exp['metrics'].update({'accuracy':accuracy})\n",
    "                else:\n",
    "                    exp['metrics'] = {'accuracy':accuracy}\n",
    "            \n",
    "        ## Precision, recall, f1\n",
    "        fpr_metrics = [metric for metric in metrics if metric in ['precision','recall','f1']]\n",
    "        if len(fpr_metrics) > 0:\n",
    "            for exp in li_exp:\n",
    "                # Calculate li_exp, recall, f1\n",
    "                precision, recall, f1, support = precision_recall_fscore_support(exp['results'][tgt_col],\n",
    "                                                                        exp['results'][pred_col_label],\n",
    "                                                                        labels=['Yes'] if average_type != 'binary' else ['Yes'],\n",
    "                                                                        #    average = None,\n",
    "                                                                            pos_label = 'Yes' if average_type == 'binary' else 1,\n",
    "                                                                            average=average_type,\n",
    "                                                                            #  average='micro'\n",
    "                                                                            zero_division=np.nan\n",
    "                                                                            )\n",
    "                \n",
    "                _ = {k:v for k,v in zip(['f1', 'precision','recall'], [f1, precision, recall]) if k in fpr_metrics}\n",
    "                # _['support'] = support\n",
    "                if 'metrics' in exp:\n",
    "                    exp['metrics'].update(_)\n",
    "                else:\n",
    "                    exp['metrics'] = _\n",
    "        \n",
    "        ## ROC_AUC\n",
    "        if 'roc_auc' in metrics:\n",
    "            for exp in li_exp:\n",
    "                # Calculate roc_auc\n",
    "                roc_auc = roc_auc_score(exp['results'][tgt_col+'_yes_prob'], exp['results']['pred_yes_prob'] )\n",
    "                if 'metrics' in exp:\n",
    "                    exp['metrics'].update({'roc_auc':roc_auc})\n",
    "                else:\n",
    "                    exp['metrics'] = {'roc_auc':roc_auc}\n",
    "        \n",
    "\n",
    "        \n",
    "        if 'mae' in metrics:\n",
    "            for exp in li_exp:\n",
    "                # Calculate calibration_error\n",
    "                \n",
    "                calibration_error = np.mean(np.abs( exp['results'][tgt_col+'_yes_prob'] -  exp['results']['pred_yes_prob'] ) )\n",
    "                \n",
    "                if 'metrics' in exp:\n",
    "                    exp['metrics'].update({'mae':calibration_error})\n",
    "                else:\n",
    "                    exp['metrics'] = {'mae':calibration_error}\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError('breakdown_by_budgetitem=True not implemented yet')\n",
    "        for exp in li_exp:\n",
    "            exp['metrics'] = {}\n",
    "            for budget_item in exp['results']['budget_item'].unique():\n",
    "                df = exp['results'][exp['results']['budget_item']==budget_item]\n",
    "                ## Accuracy\n",
    "                if 'accuracy' in metrics:\n",
    "                    # Calculate accuracy\n",
    "                    accuracy = accuracy_score(df[tgt_col], df[pred_col_label])\n",
    "                    exp['metrics'].update({f'accuracy_{budget_item}':accuracy})\n",
    "                \n",
    "                ## Precision, recall, f1\n",
    "                fpr_metrics = [metric for metric in metrics if metric in ['precision','recall','f1']]\n",
    "                if len(fpr_metrics) > 0:\n",
    "                    # Calculate li_exp, recall, f1\n",
    "                    precision, recall, f1, support = precision_recall_fscore_support(df[tgt_col],\n",
    "                                                                            df[pred_col_label],\n",
    "                                                                            labels=['Yes'] if average_type != 'binary' else None,\n",
    "                                                                            pos_label = 'Yes' if average_type == 'binary' else 1,\n",
    "                                                                                average=average_type,\n",
    "                                                                                zero_division=np.nan\n",
    "                                                                                )\n",
    "                    \n",
    "                    _ = {k:v for k,v in zip(['f1', 'precision','recall'], [f1, precision, recall]) if k in fpr_metrics}\n",
    "                    # _['support'] = support\n",
    "                    exp['metrics'].update({f'{k}_{budget_item}':v for k,v in _.items()})\n",
    "            \n",
    "                ## ROC_AUC\n",
    "                if 'roc_auc' in metrics:\n",
    "                    # Calculate roc_auc\n",
    "                    roc_auc = roc_auc_score(df[tgt_col], df[pred_col_prob], labels=['Yes','No'])\n",
    "                    exp['metrics'].update({f'roc_auc_{budget_item}':roc_auc})\n",
    "                \n",
    "                ## Brier Score\n",
    "                if 'brier_score' in metrics:\n",
    "                    # Calculate brier_score\n",
    "                    brier_score = brier_score_loss(df[tgt_col], df[pred_col_prob], pos_label='Yes')\n",
    "                    exp['metrics'].update({f'brier_score_{budget_item}':brier_score})\n",
    "\n",
    "                if 'calibration_error' in metrics:\n",
    "                    # Calculate calibration_error\n",
    "                    calibration_error = np.abs(df[tgt_col] - df[pred_col_prob])                    \n",
    "                    exp['metrics'].update({'calibration_error':calibration_error})\n",
    "\n",
    "    return li_exp\n",
    "\n",
    "def convert_li_exp_to_df(li_exp: list) -> pd.DataFrame:\n",
    "    \"\"\"Converts list of experiments to a dataframe\"\"\"\n",
    "    li_exp_metrics = []\n",
    "    for exp in li_exp:\n",
    "        \n",
    "        exp_metrics = exp['metrics']\n",
    "        \n",
    "        llm_name = exp['config']['llm_name']\n",
    "        edge_value = exp['config']['edge_value']\n",
    "        effect_type = exp['config']['effect_type']\n",
    "        finetuned = exp['config']['finetuned']\n",
    "        parse_style = exp['config']['parse_style']\n",
    "        prompt_style = exp['config']['prompt_style']\n",
    "        uc = exp['config'].get('unbias_categorisations', False)\n",
    "        exp_metrics.update({'llm_name':llm_name,\n",
    "                            'edge_value':edge_value,\n",
    "                            'effect_type':effect_type,\n",
    "                            'finetuned':finetuned,\n",
    "                            'parse_style':parse_style,\n",
    "                            'prompt_style':prompt_style,\n",
    "                            'uc':uc\n",
    "                            })\n",
    "        \n",
    "        li_exp_metrics.append(exp_metrics)\n",
    "\n",
    "    df = pd.DataFrame(li_exp_metrics)\n",
    "\n",
    "    # Put the llm_name column first\n",
    "    cols = df.columns.tolist()\n",
    "    cols.insert(0, cols.pop(cols.index('llm_name')))\n",
    "    df = df.reindex(columns=cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_diagrams_from_dataframe(df_exp: pd.DataFrame,\n",
    "                                   columns_to_create_diagrams_for=['accuracy','roc_auc','brier_score','precision','recall','f1'],\n",
    "                                   save_dir='./prompt_engineering/analysis/spot_output',\n",
    "                                   exp_name='CompareAll') -> dict:\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    save_dir = os.path.join(save_dir, exp_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Dictionary to store the paths of the saved diagrams\n",
    "    saved_diagrams = {}\n",
    "    \n",
    "    # Iterate over each column to create a diagram\n",
    "    for column in columns_to_create_diagrams_for:\n",
    "        if column in df_exp.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            df_exp[column].hist(bins=20)\n",
    "            plt.title(f'Histogram of {column}')\n",
    "            plt.xlabel(column)\n",
    "            plt.ylabel('Frequency')\n",
    "            \n",
    "            # Save the diagram\n",
    "            file_path = os.path.join(save_dir, f'{column}_histogram.png')\n",
    "            plt.savefig(file_path)\n",
    "            \n",
    "            # Store the path in the dictionary\n",
    "            saved_diagrams[column] = file_path\n",
    "            \n",
    "            # Close the plot to free up memory\n",
    "            plt.close()\n",
    "            \n",
    "    return saved_diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the results of the spot experiments\n",
    "# Directories\n",
    "dir_exp_extensive =  \"../../prompt_engineering/output/spot/extensive\"\n",
    "\n",
    "\n",
    "li_all_experiments_extensive = load_data( dir_exp_extensive )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation - Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_exp_ablation_size = '../../prompt_engineering/output/spot/ablation_size'\n",
    "li_all_experiments_ablation_size = load_data( dir_exp_ablation_size  )\n",
    "li_exps_ablation_size = calc_eval_metrics(li_all_experiments_ablation_size, metrics=['f1','mae','roc_auc'] )\n",
    "df_exps_ablation_size = convert_li_exp_to_df(li_exps_ablation_size)\n",
    "display(df_exps_ablation_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Style CPUQ - With or without Question & reasonsing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ablation_reasoning_step = \"../../prompt_engineering/output/spot/abaltion_reasoning_step\"\n",
    "li_all_experiments_ablation_reasoning_step = load_data(dir_ablation_reasoning_step)\n",
    "li_exps_ablation_reasoning_step = filter_exp(li_all_experiments_ablation_reasoning_step )\n",
    "\n",
    "li_exps_ablation_reasoning_step_w_res = calc_eval_metrics(li_exps_ablation_reasoning_step, metrics=['f1','mae','roc_auc'] )\n",
    "\n",
    "\n",
    "df_exps_ablation_reasoning_step = convert_li_exp_to_df(li_exps_ablation_reasoning_step_w_res)\n",
    "display(df_exps_ablation_reasoning_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics for binary vs prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ablation_binary_vs_prob  = \"../../prompt_engineering/output/spot/ablation_binary_vs_prob\"\n",
    "li_all_experiments_ablation_binary_vs_prob = load_data(dir_ablation_binary_vs_prob)\n",
    "\n",
    "\n",
    "li_exps_ablation_binary_vs_prob_w_res = calc_eval_metrics(li_all_experiments_ablation_binary_vs_prob, metrics=['f1','mae','roc_auc'] )\n",
    "\n",
    "\n",
    "df_exps_ablation_binary_vs_prob = convert_li_exp_to_df(li_exps_ablation_binary_vs_prob_w_res)\n",
    "display(df_exps_ablation_binary_vs_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation naive prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ablation_naive_prompting = \"../../prompt_engineering/output/spot/extensive\"\n",
    "li_all_experiments_ablation_naive_prompting = load_data(dir_ablation_naive_prompting)\n",
    "li_exps_ablation_naive_prompting = filter_exp(li_all_experiments_ablation_naive_prompting, prompt_style=['yes_no']  )\n",
    "\n",
    "li_exps_ablation_naive_prompting_w_res = calc_eval_metrics(li_exps_ablation_naive_prompting, metrics=['f1','mae','roc_auc'] )\n",
    "\n",
    "df_exps_ablation_naive_prompting = convert_li_exp_to_df(li_exps_ablation_naive_prompting_w_res)\n",
    "display(df_exps_ablation_naive_prompting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying Unbiased Categorisations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics= ['f1','mae','roc_auc','accuracy','precision','recall','calibration_error']\n",
    "\n",
    "dir_ablation_uc = \"../../prompt_engineering/output/spot/ablation_uc\"\n",
    "li_all_experiments_ablation_uc = load_data(dir_ablation_uc)\n",
    "li_exps_ablation_uc = filter_exp(li_all_experiments_ablation_uc )\n",
    "\n",
    "li_exps_ablation_uc_w_res = calc_eval_metrics(li_exps_ablation_uc, metrics=metrics )\n",
    "\n",
    "df_exps_ablation_uc = convert_li_exp_to_df(li_exps_ablation_uc_w_res)\n",
    "display(df_exps_ablation_uc)\n",
    "\n",
    "\n",
    "# 3. Filtering the data and calculating the performance differences\n",
    "df = df_exps_ablation_uc\n",
    "# rip model size from llm_name as 3B is 3bn, 7B is 7bn, 13B is 13bn, 30B is 30bn e.g. just search for that tag and then insert 1bn or 3bn or70bn. Chekc if the subprhase is in the name\n",
    "df['model_size'] = df['llm_name'].apply(lambda x: re.search(r'\\d+B', x).group(0).replace('B', 'bn'))\n",
    "\n",
    "model_sizes = ['3bn', '8bn','70bn']\n",
    "filtered_df = df[(df['uc'].isin([False,True])) & \n",
    "                 (df['model_size'].isin(model_sizes))]\n",
    "\n",
    "diff_df = pd.DataFrame(columns=[\"model_size\", \"metric\", \"uc\"])\n",
    "for model in ['3bn', '8bn','70bn']:\n",
    "\n",
    "    for metric in metrics\n",
    "    non_uc_f1 = filtered_df[(filtered_df['model_size'] == model) & \n",
    "                                   (filtered_df['uc'] == False)]['f1'].values[0][0]\n",
    "\n",
    "    non_uc_mae = filtered_df[(filtered_df['model_size'] == model) & \n",
    "                                   (filtered_df['uc'] == False)]['mae'].values[0]\n",
    "\n",
    "    non_uc_roc_auc = filtered_df[(filtered_df['model_size'] == model) & \n",
    "                                   (filtered_df['uc'] == False)]['roc_auc'].values[0]    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    uc_f1_diff = filtered_df[(filtered_df['model_size'] == model) & \n",
    "                                         (filtered_df['uc'] == True )]['f1'].values[0][0] - non_uc_f1\n",
    "    \n",
    "    uc_mae_diff = filtered_df[(filtered_df['model_size'] == model) & \n",
    "                                         (filtered_df['uc'] == True )]['mae'].values[0] - non_uc_mae\n",
    "\n",
    "    uc_roc_auc_diff = filtered_df[(filtered_df['model_size'] == model) & \n",
    "                                            (filtered_df['uc'] == True )]['roc_auc'].values[0] - non_uc_roc_auc\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # diff_df = diff_df.append({\"model_size\": model, \"metric\": \"f1\", \"uc\": uc_f1_diff }, ignore_index=True)\n",
    "    # diff_df = diff_df.append({\"model_size\": model, \"metric\": \"mae\", \"uc\": uc_mae_diff,}, ignore_index=True)\n",
    "    # diff_df = diff_df.append({\"model_size\": model, \"metric\": \"roc_auc\", \"uc\": uc_roc_auc_diff }, ignore_index=True)\n",
    "    # Use pd.concat instead of append\n",
    "    new_rows = pd.DataFrame([\n",
    "        {\"model_size\": model, \"metric\": \"f1\", \"uc\": uc_f1_diff},\n",
    "        {\"model_size\": model, \"metric\": \"mae\", \"uc\": uc_mae_diff},\n",
    "        {\"model_size\": model, \"metric\": \"roc_auc\", \"uc\": uc_roc_auc_diff}\n",
    "    ])\n",
    "    diff_df = pd.concat([diff_df, new_rows], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# 4. Plotting the differences in a bar chart with reduced gap\n",
    "barWidth = 0.7\n",
    "gap = 0.7\n",
    "r1 = np.arange(0, 2.8*len(diff_df[diff_df['metric'] == 'f1']) - gap, 2.8)\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + 2*barWidth for x in r1]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "bars1 = ax.bar(r1, diff_df[diff_df['metric'] == 'f1']['uc'].values  , width=barWidth, label='f1', color='b')\n",
    "bars2 = ax.bar(r2, diff_df[diff_df['metric'] == 'mae']['uc'].values, width=barWidth, label='mae', color='r')\n",
    "bars3 = ax.bar(r3, diff_df[diff_df['metric'] == 'roc_auc']['uc'].values, width=barWidth, label='roc_auc', color='c')\n",
    "\n",
    "def label_bars(bars):\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        if yval > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 3), ha='center', va='bottom')\n",
    "        else:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 3), ha='center', va='top')\n",
    "\n",
    "label_bars(bars1)\n",
    "label_bars(bars2)\n",
    "label_bars(bars3)\n",
    "# ax.set_title('Difference in Performance Metrics Due to Unbiased Categorisation')\n",
    "ax.set_xlabel('Model Size', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('Difference', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks([(r + 1.1*barWidth) for r in r1])\n",
    "ax.set_xticklabels(model_sizes, fontsize=12)\n",
    "ax.legend(fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "import os\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "fig.savefig('figures/ablation_alignment_unbiased_categorisation.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics= ['f1','mae','roc_auc','accuracy','precision','recall']\n",
    "# metrics= ['f1','mae']\n",
    "\n",
    "dir_ablation_uc = \"../../prompt_engineering/output/spot/ablation_uc\"\n",
    "li_all_experiments_ablation_uc = load_data(dir_ablation_uc)\n",
    "li_exps_ablation_uc = filter_exp(li_all_experiments_ablation_uc )\n",
    "\n",
    "li_exps_ablation_uc_w_res = calc_eval_metrics(li_exps_ablation_uc, metrics=metrics )\n",
    "\n",
    "df_exps_ablation_uc = convert_li_exp_to_df(li_exps_ablation_uc_w_res)\n",
    "display(df_exps_ablation_uc)\n",
    "\n",
    "# 3. Filtering the data and calculating the performance differences\n",
    "df = df_exps_ablation_uc\n",
    "# rip model size from llm_name as 3B is 3bn, 7B is 7bn, 13B is 13bn, 30B is 30bn e.g. just search for that tag and then insert 1bn or 3bn or70bn. Chekc if the subprhase is in the name\n",
    "df['model_size'] = df['llm_name'].apply(lambda x: re.search(r'\\d+B', x).group(0).replace('B', 'bn'))\n",
    "\n",
    "model_sizes = ['3bn', '8bn','70bn']\n",
    "filtered_df = df[(df['uc'].isin([False,True])) & \n",
    "                 (df['model_size'].isin(model_sizes))]\n",
    "\n",
    "diff_df = pd.DataFrame(columns=[\"model_size\", \"metric\", \"uc\"])\n",
    "for model in ['3bn', '8bn','70bn']:\n",
    "    for metric in metrics:\n",
    "        # Get non-UC value\n",
    "        non_uc_value = filtered_df[(filtered_df['model_size'] == model) & \n",
    "                                 (filtered_df['uc'] == False)][metric].values[0]\n",
    "        \n",
    "        # Get UC value\n",
    "        uc_value = filtered_df[(filtered_df['model_size'] == model) & \n",
    "                             (filtered_df['uc'] == True)][metric].values[0]\n",
    "        \n",
    "        # Handle f1 which is stored as a list\n",
    "        # if metric == 'f1':\n",
    "        #     non_uc_value = non_uc_value[0]\n",
    "        #     uc_value = uc_value[0]\n",
    "\n",
    "        # Handle metrics that are stored as lists or arrays\n",
    "        if isinstance(non_uc_value, (list, np.ndarray)):\n",
    "            non_uc_value = non_uc_value[0]\n",
    "        if isinstance(uc_value, (list, np.ndarray)):\n",
    "            uc_value = uc_value[0]\n",
    "        \n",
    "        # Calculate difference\n",
    "        diff = uc_value - non_uc_value\n",
    "        \n",
    "        # Add to dataframe\n",
    "        new_row = pd.DataFrame([{\n",
    "            \"model_size\": model,\n",
    "            \"metric\": metric,\n",
    "            \"uc\": diff\n",
    "        }])\n",
    "        diff_df = pd.concat([diff_df, new_row], ignore_index=True)\n",
    "\n",
    "# Update plotting code to handle all metrics\n",
    "barWidth =0.3\n",
    "gap = 0.9\n",
    "num_metrics = len(metrics)\n",
    "r_base = np.arange(0, 2.8*len(model_sizes) - gap, 2.8)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "bars = []\n",
    "colors = plt.cm.get_cmap('tab20')(np.linspace(0, 1, num_metrics))\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    r = [x + idx*barWidth for x in r_base]\n",
    "    metric_data = diff_df[diff_df['metric'] == metric]['uc'].values\n",
    "    bars.append(ax.bar(r, metric_data, width=barWidth, label=metric, color=colors[idx]))\n",
    "\n",
    "# Label bars function remains the same\n",
    "def label_bars(bars):\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        if yval > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 3), ha='center', va='bottom')\n",
    "        else:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 3), ha='center', va='top')\n",
    "\n",
    "for bar_group in bars:\n",
    "    label_bars(bar_group)\n",
    "\n",
    "ax.set_xlabel('Model Size', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('Difference', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks([r + (num_metrics-1)*barWidth/2 for r in r_base])\n",
    "ax.set_xticklabels(model_sizes, fontsize=12)\n",
    "ax.legend(fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('figures/ablation_alignment_unbiased_categorisation.png', dpi=300)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alanturing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc50b8d0f7e8d8fb88331066a22b4d3e98f2c46ac896a3cf7dc663ff0af4e185"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
